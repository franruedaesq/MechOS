# MechOS

MechOS is a strict, offline-first Agent Operating System designed specifically for physical robotics. It serves as the critical bridge between high-level, non-deterministic cognitive reasoning (local LLMs like Ollama) and low-level, deterministic hardware execution (ROS2 and physical actuators).

Unlike a simple script or a web-based agent framework, MechOS acts as a true Operating System. It treats the LLM as the "brain" and the hardware as the "muscle," utilizing a central Kernel to enforce strict physical safety boundaries, manage permissions, and prevent catastrophic hardware failures.

---

## Core Architectural Principles

1. **Modular Monolith & Cargo Workspace:** MechOS is built in Rust using a Cargo Workspace. The system is split into distinct, decoupled crates with clear boundaries. While it compiles into a single, highly performant native binary for deployment, the crates themselves are entirely independent.
2. **Ecosystem Portability (NPM/TypeScript):** Because the crates are independent, modules like the perception engine or ROS2 bridge can be compiled via WebAssembly or `napi-rs` and published as standalone NPM packages for TypeScript developers.
3. **Separation of Orchestration and Execution:** The architecture strictly separates the **Kernel** (which manages safety policies and permissions) from the **Runtime** (which executes the AI's OODA loop and talks to the LLM). If the LLM hallucinates or crashes, the Kernel catches the fault before physical harm occurs.

---

## Crate Structure and Responsibilities

The workspace is divided into six core crates, with dependencies flowing strictly downward.

### 1. `mechos-types` (The Foundation)

This crate has zero internal dependencies and defines the shared vocabulary of the OS.

* **Capabilities:** Defines the strict permissions an agent can hold, such as `HardwareInvoke("drive_base")` or `SensorRead("lidar")`.
* **Intents & Events:** Contains the `HardwareIntent` enums (the exact physical actions the LLM is allowed to request) and the `EventPayload` structs for internal messaging.

### 2. `mechos-middleware` (The Nervous System)

This crate routes asynchronous data between the hardware, the OS kernel, and external clients without parsing the underlying meaning.

* **Universal ROS2 Bridge:** A middleware translation layer that converts heavy DDS robotics traffic into lightweight JSON, allowing the LLM and web clients to read sensor data seamlessly.
* **Headless Event Bus:** A typed, topic-based publish/subscribe system for inter-crate communication.

### 3. `mechos-hal` (Hardware Abstraction Layer)

The physical boundary of the OS. It translates the abstract intents generated by the LLM into electrical signals.

* **Generic Interfaces:** Exposes hardware capabilities (actuators, cameras, relays) through a uniform API, allowing drivers to be swapped without changing the AI logic.
* **Generic PID Controller Engine:** Tunable feedback control loops that minimize physical error over time, ensuring smooth motor movements without requiring micro-management from the LLM.

### 4. `mechos-perception` (Embodied Cognition)

LLMs require a mathematical representation of the physical world. This crate turns noisy sensor data into actionable state.

* **Transform Frame (TF) Engine:** A directed graph computing spatial transforms (translations, rotations) between named reference frames.
* **Sensor Fusion Engine:** Combines heterogeneous data streams (e.g., Odometry + IMU) into a unified state estimate.
* **Spatial Query & Collision Engine:** Uses Octrees to partition 3D space, providing fast collision detection so the LLM knows if a path is clear.

### 5. `mechos-memory` (The Knowledge Base)

Provides the robot with persistent state and recall capabilities, utilizing a local SQLite substrate.

* **Episodic Memory Store:** A local vector database (`EpisodicStore`) that persists interaction summaries together with their dense embedding vectors to SQLite and supports cosine-similarity–based recall so the runtime can retrieve the memories most semantically relevant to a query.
* **Semantic Vector State Estimator:** (`SemanticStateEstimator`) Fuses past visual embeddings with a time-decay probability model to track the semantic state of the world over time (e.g., remembering where an object was placed). Confidence rises on fresh observations and decays exponentially between ticks.

### 6. `mechos-kernel` (Safety & Orchestration)

The central brainstem. It does not think; it enforces rules and regulates the system.

* **Capability Manager:** Enforces the principle of least privilege. Before any tool or hardware is invoked, the Kernel verifies the agent holds the correct `Capability`.
* **State Verifier / Safety Interlock:** A rule engine that continuously monitors physical invariants (like joint limits or speed caps) and triggers fallback behaviors if violated.
* **Watchdog / Health Monitor:** Tracks heartbeats from all components and triggers restarts if a subsystem freezes.

### 7. `mechos-runtime` (The AI Brain)

The execution engine where the "thinking" happens, implementing the Observe-Orient-Decide-Act (OODA) loop.

* **LLM Driver Abstraction:** Uses an OpenAI-compatible interface to communicate with local models like Ollama (`http://localhost:11434`).
* **Behavior Tree Engine:** An executor for a composable tree of Sequence, Selector, and Leaf nodes. The LLM selects high-level behaviors rather than controlling raw motor ticks.
* **Loop Guard:** A safety mechanism that detects if the LLM is stuck in a repetitive loop (e.g., repeatedly requesting a failing action) and forces an intervention.

---

## The Execution Flow (From Cognition to Actuation)

1. **Observe:** `mechos-middleware` ingests ROS2 data. `mechos-perception` processes this data into a spatial map and checks for collisions.
2. **Orient:** `mechos-runtime` builds a context window for the LLM, combining the spatial summary with historical context.
3. **Decide:** The LLM evaluates the prompt and returns a JSON payload selecting a specific hardware tool or Behavior Tree.
4. **Gatekeep:** `mechos-kernel` intercepts the request. It checks the `CapabilityManager` for permissions and runs the `State Verifier` to ensure the requested action will not violate physical safety invariants.
5. **Act:** If approved, the intent is passed to `mechos-hal`, which utilizes PID controllers to send the final electrical signals to the physical actuators.

---

## Getting Started

### Prerequisites

* [Rust](https://rustup.rs/) (edition 2024, stable toolchain)

### Build

```bash
cargo build
```

### Test

```bash
cargo test
```

---

## Workspace Layout

```
mechos/
├── Cargo.toml                  # Workspace manifest
└── crates/
    ├── mechos-types/           # Shared types, capabilities, errors
    ├── mechos-middleware/      # ROS2 bridge & event bus
    ├── mechos-hal/             # Hardware abstraction layer
    ├── mechos-perception/      # Sensor fusion & spatial reasoning
    ├── mechos-memory/          # Episodic memory store & semantic state estimator
    ├── mechos-kernel/          # Safety, permissions, watchdog
    └── mechos-runtime/         # OODA loop & LLM driver
```
