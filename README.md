# MechOS

MechOS is a strict, offline-first Agent Operating System designed specifically for physical robotics. It serves as the critical bridge between high-level, non-deterministic cognitive reasoning (local LLMs like Ollama) and low-level, deterministic hardware execution (ROS2 and physical actuators).

Unlike a simple script or a web-based agent framework, MechOS acts as a true Operating System. It treats the LLM as the "brain" and the hardware as the "muscle," utilizing a central Kernel to enforce strict physical safety boundaries, manage permissions, and prevent catastrophic hardware failures.

---

## The Core Philosophy

**MechOS is a Cognitive Brain, not a physics engine.** LLMs are terrible at calculus and geometry. Therefore, MechOS must output *high-level spatial intents* (e.g., "Move hand to X, Y, Z"), and the Universal Integration Layer must translate those intents into low-level math (Inverse Kinematics) and physical motor currents.

This layer completely decouples the AI logic from the hardware, allowing MechOS to drive a real robot via ROS 2, or a virtual robot in a React/Three.js web dashboard, using the exact same codebase.

---

## Core Architectural Principles

1. **Modular Monolith & Cargo Workspace:** MechOS is built in Rust using a Cargo Workspace. The system is split into distinct, decoupled crates with clear boundaries. While it compiles into a single, highly performant native binary for deployment, the crates themselves are entirely independent.
2. **Ecosystem Portability (NPM/TypeScript):** Because the crates are independent, modules like the perception engine or ROS2 bridge can be compiled via WebAssembly or `napi-rs` and published as standalone NPM packages for TypeScript developers.
3. **Separation of Orchestration and Execution:** The architecture strictly separates the **Kernel** (which manages safety policies and permissions) from the **Runtime** (which executes the AI's OODA loop and talks to the LLM). If the LLM hallucinates or crashes, the Kernel catches the fault before physical harm occurs.
4. **Universal Integration Layer:** Hardware adapters translate high-level `HardwareIntent` commands (e.g., `MoveEndEffector { x, y, z }`) into low-level math (Inverse Kinematics) and motor currents, keeping all geometric computation outside the LLM.

---

## Crate Structure and Responsibilities

The workspace is divided into seven core crates, with dependencies flowing strictly downward.

### 1. `mechos-types` (The Foundation)

This crate has zero internal dependencies and defines the shared vocabulary of the OS.

* **Capabilities:** Defines the strict permissions an agent can hold, such as `HardwareInvoke("drive_base")` or `SensorRead("lidar")`.
* **Intents & Events:** Contains the `HardwareIntent` enum (the exact physical actions the LLM is allowed to request) and the `EventPayload` structs for internal messaging. The enum derives `JsonSchema` via `schemars` so a JSON Schema can be automatically generated and injected into LLM requests to force strictly typed outputs.

#### HardwareIntent Variants

| Variant | Description |
|---------|-------------|
| `MoveEndEffector { x, y, z }` | High-level spatial command. The Universal Integration Adapter resolves Inverse Kinematics. |
| `Drive { linear_velocity, angular_velocity }` | Low-level differential drive. |
| `TriggerRelay { relay_id, state }` | Discrete on/off hardware action. |
| `AskHuman { question, context_image_id }` | HITL – the AI is uncertain and requests human guidance via the Dashboard. |

### 2. `mechos-middleware` (The Nervous System)

This crate routes asynchronous data between the hardware, the OS kernel, and external clients without parsing the underlying meaning.

* **Universal ROS2 Bridge:** A middleware translation layer that converts heavy DDS robotics traffic into lightweight JSON, allowing the LLM and web clients to read sensor data seamlessly.
* **Headless Event Bus:** A typed, topic-based publish/subscribe system for inter-crate communication.

### 3. `mechos-hal` (Hardware Abstraction Layer)

The physical boundary of the OS. It translates the abstract intents generated by the LLM into electrical signals.

* **Generic Interfaces:** Exposes hardware capabilities (actuators, cameras, relays) through a uniform API, allowing drivers to be swapped without changing the AI logic.
* **Generic PID Controller Engine:** Tunable feedback control loops that minimize physical error over time, ensuring smooth motor movements without requiring micro-management from the LLM.
* **Universal Integration Point:** `MoveEndEffector` is dispatched to the registered `end_effector` actuator; the external IK adapter is responsible for translating 3-D coordinates into joint angles before reaching the registry.

### 4. `mechos-perception` (Embodied Cognition)

LLMs require a mathematical representation of the physical world. This crate turns noisy sensor data into actionable state.

* **Transform Frame (TF) Engine:** A directed graph computing spatial transforms (translations, rotations) between named reference frames.
* **Sensor Fusion Engine:** Combines heterogeneous data streams (e.g., Odometry + IMU) into a unified state estimate.
* **Spatial Query & Collision Engine:** Uses Octrees to partition 3D space, providing fast collision detection so the LLM knows if a path is clear.

### 5. `mechos-memory` (The Knowledge Base)

Provides the robot with persistent state and recall capabilities, utilizing a local SQLite substrate.

* **Episodic Memory Store:** A local vector database (`EpisodicStore`) that persists interaction summaries together with their dense embedding vectors to SQLite and supports cosine-similarity–based recall so the runtime can retrieve the memories most semantically relevant to a query.
* **Semantic Vector State Estimator:** (`SemanticStateEstimator`) Fuses past visual embeddings with a time-decay probability model to track the semantic state of the world over time. Confidence rises on fresh observations and decays exponentially between ticks.

### 6. `mechos-kernel` (Safety & Orchestration)

The central brainstem. It does not think; it enforces rules and regulates the system.

* **Capability Manager:** Enforces the principle of least privilege. Before any tool or hardware is invoked, the Kernel verifies the agent holds the correct `Capability`.
* **State Verifier / Safety Interlock:** A rule engine that continuously monitors physical invariants (workspace bounds, speed caps) and triggers fallback behaviors if violated.
* **Watchdog / Health Monitor:** Tracks heartbeats from all components and triggers restarts if a subsystem freezes.

### 7. `mechos-runtime` (The AI Brain)

The execution engine where the "thinking" happens, implementing the Observe-Orient-Decide-Act (OODA) loop.

* **Definitive OODA Orchestrator (`AgentLoop`):** The infinite asynchronous loop that drives the robot:
  - **Observe** – queries `mechos-perception` for the latest `FusedState` (odometry) and collision data from the Octree.
  - **Orient** – formats state into a strict system prompt and retrieves relevant memories from the `EpisodicStore`.
  - **Decide** – calls `LlmDriver.complete()`. The returned JSON is hashed and checked against `LoopGuard` to ensure the agent isn't stuck in a repetitive hallucination loop.
  - **Gatekeep** – the parsed `HardwareIntent` is checked by `CapabilityManager` (permission) and `StateVerifier` (physical invariants) via `KernelGate`.
  - **Act** – the approved intent is published to the `EventBus`.
* **Structured LLM Outputs:** The `LlmDriver` automatically derives a JSON Schema from the `HardwareIntent` enum using the `schemars` crate and injects it into every Ollama/OpenAI API request via `response_format: { type: "json_schema" }`. This forces the LLM to output strictly typed JSON that maps directly to Rust structs.
* **Behavior Tree Engine:** An executor for a composable tree of Sequence, Selector, and Leaf nodes. The LLM selects high-level behaviors rather than controlling raw motor ticks.
* **Loop Guard:** A safety mechanism that detects if the LLM is stuck in a repetitive loop and forces an intervention.

---

## The Execution Flow (From Cognition to Actuation)

1. **Observe:** `mechos-middleware` ingests ROS2 data. `mechos-perception` processes this into a spatial map and checks for collisions.
2. **Orient:** `mechos-runtime` builds a context window for the LLM, combining the spatial summary with episodic memories from `mechos-memory`.
3. **Decide:** The LLM evaluates the prompt and returns a JSON payload conforming to the `HardwareIntent` JSON Schema (enforced via `response_format`). The `LoopGuard` hashes the output and detects repetitive hallucinations.
4. **Gatekeep:** `mechos-kernel` intercepts the request. It checks the `CapabilityManager` for permissions and runs the `StateVerifier` to ensure the requested action will not violate physical safety invariants.
5. **Act:** If approved, the intent is published to the `EventBus` and forwarded to `mechos-hal`, which utilizes PID controllers to send the final electrical signals to the physical actuators.

---

## Getting Started

### Prerequisites

* [Rust](https://rustup.rs/) (edition 2024, stable toolchain)

### Build

```bash
cargo build
```

### Test

```bash
cargo test
```

---

## Workspace Layout

```
mechos/
├── Cargo.toml                  # Workspace manifest
└── crates/
    ├── mechos-types/           # Shared types, capabilities, errors
    ├── mechos-middleware/      # ROS2 bridge & event bus
    ├── mechos-hal/             # Hardware abstraction layer
    ├── mechos-perception/      # Sensor fusion & spatial reasoning
    ├── mechos-memory/          # Episodic memory store & semantic state estimator
    ├── mechos-kernel/          # Safety, permissions, watchdog
    └── mechos-runtime/         # OODA loop & LLM driver
```
